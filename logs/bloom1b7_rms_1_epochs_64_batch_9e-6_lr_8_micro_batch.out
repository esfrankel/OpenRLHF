[2024-07-01 16:50:06,050] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:06,106] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:06,106] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:27,651] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,657] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 64 --micro_train_batch_size 8 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset argilla/ultrafeedback-binarized-preferences-cleaned --dataset_probs 1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
[2024-07-01 16:50:27,656] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-01 16:50:27,656] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,677] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 64 --micro_train_batch_size 8 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset argilla/ultrafeedback-binarized-preferences-cleaned, --dataset_probs 1.0,1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,677] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 64 --micro_train_batch_size 8 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset --dataset_probs 1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
[2024-07-01 16:50:29,784] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,061] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,470] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:30,478] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:30,478] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:30,478] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:30,478] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:30,479] [INFO] [launch.py:253:main] process 41405 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:30,639] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,771] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:30,772] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:30,772] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:30,772] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:30,772] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:30,773] [INFO] [launch.py:253:main] process 26955 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned,', '--dataset_probs', '1.0,1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:31,370] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:31,370] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:31,370] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:31,370] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:31,371] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:31,371] [INFO] [launch.py:253:main] process 26963 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:46,315] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:46,375] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:46,375] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:52,821] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-01 16:50:52,825] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-01 16:50:52,830] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-01 16:50:52,842] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-01 16:50:54,269] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 230, num_elems = 5.26B
[2024-07-01 16:50:54,286] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 230, num_elems = 5.26B
[2024-07-01 16:50:57,397] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 26963
[2024-07-01 16:50:57,403] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 2
[2024-07-01 16:50:59,509] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 41405
[2024-07-01 16:50:59,513] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 1
[2024-07-01 16:51:07,812] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 26955
[2024-07-01 16:51:07,817] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned,', '--dataset_probs', '1.0,1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 1
