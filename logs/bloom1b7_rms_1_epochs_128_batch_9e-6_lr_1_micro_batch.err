/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
usage: train_rm.py [-h] [--pretrain PRETRAIN] [--dataset DATASET]
                   [--dataset_probs DATASET_PROBS] [--save_path SAVE_PATH]
                   [--save_steps SAVE_STEPS] [--logging_steps LOGGING_STEPS]
                   [--eval_steps EVAL_STEPS] [--ckpt_path CKPT_PATH]
                   [--max_ckpt_num MAX_CKPT_NUM] [--max_ckpt_mem MAX_CKPT_MEM]
                   [--max_epochs MAX_EPOCHS]
                   [--micro_train_batch_size MICRO_TRAIN_BATCH_SIZE]
                   [--train_batch_size TRAIN_BATCH_SIZE]
                   [--max_samples MAX_SAMPLES] [--load_checkpoint]
                   [--max_norm MAX_NORM] [--max_len MAX_LEN] [--l2 L2]
                   [--loss LOSS] [--gradient_checkpointing] [--seed SEED]
                   [--local_rank LOCAL_RANK] [--zero_stage ZERO_STAGE]
                   [--bf16] [--learning_rate LEARNING_RATE] [--zpg ZPG]
                   [--adam_offload] [--flash_attn] [--compute_fp32_loss]
                   [--margin_loss] [--aux_loss_coef AUX_LOSS_COEF]
                   [--grad_accum_dtype GRAD_ACCUM_DTYPE]
                   [--disable_trace_cache] [--load_in_4bit]
                   [--lora_rank LORA_RANK] [--lora_alpha LORA_ALPHA]
                   [--lora_dropout LORA_DROPOUT]
                   [--target_modules [TARGET_MODULES ...]]
                   [--gradient_checkpointing_use_reentrant]
                   [--disable_fast_tokenizer] [--head_prefix HEAD_PREFIX]
                   [--prompt_key PROMPT_KEY] [--chosen_key CHOSEN_KEY]
                   [--rejected_key REJECTED_KEY]
                   [--input_template INPUT_TEMPLATE] [--apply_chat_template]
                   [--tokenizer_chat_template TOKENIZER_CHAT_TEMPLATE]
                   [--use_wandb USE_WANDB] [--wandb_org WANDB_ORG]
                   [--wandb_group WANDB_GROUP] [--wandb_project WANDB_PROJECT]
                   [--wandb_run_name WANDB_RUN_NAME]
train_rm.py: error: argument --dataset: expected one argument
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 184, in <module>
    train(args)
  File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 18, in train
    strategy.setup_distributed()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/utils/deepspeed.py", line 81, in setup_distributed
    deepspeed.init_distributed(timeout=timeout)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 121, in __init__
Traceback (most recent call last):
  File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 184, in <module>
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 149, in init_process_group
    train(args)
  File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 18, in train
    strategy.setup_distributed()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/utils/deepspeed.py", line 81, in setup_distributed
    deepspeed.init_distributed(timeout=timeout)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 121, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 149, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
        store, rank, world_size = next(rendezvous_iterator)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
torch.distributed.init_process_group(backend,
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
    return func(*args, **kwargs)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    return TCPStore(
torch.distributed    func_return = func(*args, **kwargs)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
    store, rank, world_size = next(rendezvous_iterator)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x147cf0ed2f80>
Traceback (most recent call last):
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 444, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 422, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_4d_kernel", __class__._4d_kernel)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 150, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 69, in put
    os.rename(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_4d_kernel.pickle.tmp' -> '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_4d_kernel.pickle'
