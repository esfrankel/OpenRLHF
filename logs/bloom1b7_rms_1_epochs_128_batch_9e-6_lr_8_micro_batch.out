[2024-07-01 16:50:06,050] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:06,105] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:06,105] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:27,646] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,651] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 128 --micro_train_batch_size 8 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset argilla/ultrafeedback-binarized-preferences-cleaned, --dataset_probs 1.0,1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
[2024-07-01 16:50:27,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-01 16:50:27,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,669] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 128 --micro_train_batch_size 8 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset argilla/ultrafeedback-binarized-preferences-cleaned --dataset_probs 1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,669] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 128 --micro_train_batch_size 8 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset --dataset_probs 1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
[2024-07-01 16:50:29,711] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,040] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,048] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,439] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:30,444] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:30,444] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:30,444] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:30,445] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:30,445] [INFO] [launch.py:253:main] process 55884 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '128', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned,', '--dataset_probs', '1.0,1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:30,825] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:30,831] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:30,828] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:30,831] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:30,831] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:30,831] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:30,831] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:30,831] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:30,831] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:30,831] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:30,832] [INFO] [launch.py:253:main] process 80869 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '128', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:30,832] [INFO] [launch.py:253:main] process 80870 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '128', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:46,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:46,359] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:46,359] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:52,821] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-01 16:50:52,824] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-01 16:50:52,830] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-01 16:50:52,836] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-01 16:50:54,270] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 230, num_elems = 5.26B
[2024-07-01 16:50:54,282] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 230, num_elems = 5.26B
[2024-07-01 16:51:01,863] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 80870
[2024-07-01 16:51:01,865] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 80869
[2024-07-01 16:51:01,865] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '128', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 2
[2024-07-01 16:51:01,865] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '128', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 1
[2024-07-01 16:51:03,479] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 55884
[2024-07-01 16:51:03,484] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_128_batch_9e-6_lr_8_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '128', '--micro_train_batch_size', '8', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned,', '--dataset_probs', '1.0,1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 1
