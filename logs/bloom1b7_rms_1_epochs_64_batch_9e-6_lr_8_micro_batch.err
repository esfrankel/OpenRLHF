/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
usage: train_rm.py [-h] [--pretrain PRETRAIN] [--dataset DATASET]
                   [--dataset_probs DATASET_PROBS] [--save_path SAVE_PATH]
                   [--save_steps SAVE_STEPS] [--logging_steps LOGGING_STEPS]
                   [--eval_steps EVAL_STEPS] [--ckpt_path CKPT_PATH]
                   [--max_ckpt_num MAX_CKPT_NUM] [--max_ckpt_mem MAX_CKPT_MEM]
                   [--max_epochs MAX_EPOCHS]
                   [--micro_train_batch_size MICRO_TRAIN_BATCH_SIZE]
                   [--train_batch_size TRAIN_BATCH_SIZE]
                   [--max_samples MAX_SAMPLES] [--load_checkpoint]
                   [--max_norm MAX_NORM] [--max_len MAX_LEN] [--l2 L2]
                   [--loss LOSS] [--gradient_checkpointing] [--seed SEED]
                   [--local_rank LOCAL_RANK] [--zero_stage ZERO_STAGE]
                   [--bf16] [--learning_rate LEARNING_RATE] [--zpg ZPG]
                   [--adam_offload] [--flash_attn] [--compute_fp32_loss]
                   [--margin_loss] [--aux_loss_coef AUX_LOSS_COEF]
                   [--grad_accum_dtype GRAD_ACCUM_DTYPE]
                   [--disable_trace_cache] [--load_in_4bit]
                   [--lora_rank LORA_RANK] [--lora_alpha LORA_ALPHA]
                   [--lora_dropout LORA_DROPOUT]
                   [--target_modules [TARGET_MODULES ...]]
                   [--gradient_checkpointing_use_reentrant]
                   [--disable_fast_tokenizer] [--head_prefix HEAD_PREFIX]
                   [--prompt_key PROMPT_KEY] [--chosen_key CHOSEN_KEY]
                   [--rejected_key REJECTED_KEY]
                   [--input_template INPUT_TEMPLATE] [--apply_chat_template]
                   [--tokenizer_chat_template TOKENIZER_CHAT_TEMPLATE]
                   [--use_wandb USE_WANDB] [--wandb_org WANDB_ORG]
                   [--wandb_group WANDB_GROUP] [--wandb_project WANDB_PROJECT]
                   [--wandb_run_name WANDB_RUN_NAME]
train_rm.py: error: argument --dataset: expected one argument
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 184, in <module>
[rank0]:     train(args)
[rank0]:   File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 22, in train
[rank0]:     model = get_llm_for_sequence_regression(
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/models/model.py", line 113, in get_llm_for_sequence_regression
[rank0]:     model = cls_class.from_pretrained(
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3626, in from_pretrained
[rank0]:     model = cls(config, *model_args, **model_kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/models/model.py", line 172, in __init__
[rank0]:     setattr(self, self.base_model_prefix, base_llm_model(config))
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 878, in __init__
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 878, in <listcomp>
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 680, in __init__
[rank0]:     self.mlp = LlamaMLP(config)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 193, in __init__
[rank0]:     self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 98, in __init__
[rank0]:     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 238, in wrapped_fn
[rank0]:     tensor: Tensor = fn(*args, **kwargs)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 184, in <module>
[rank0]:     train(args)
[rank0]:   File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 22, in train
[rank0]:     model = get_llm_for_sequence_regression(
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/models/model.py", line 113, in get_llm_for_sequence_regression
[rank0]:     model = cls_class.from_pretrained(
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3626, in from_pretrained
[rank0]:     model = cls(config, *model_args, **model_kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/models/model.py", line 172, in __init__
[rank0]:     setattr(self, self.base_model_prefix, base_llm_model(config))
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 878, in __init__
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 878, in <listcomp>
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 680, in __init__
[rank0]:     self.mlp = LlamaMLP(config)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 193, in __init__
[rank0]:     self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 98, in __init__
[rank0]:     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 238, in wrapped_fn
[rank0]:     tensor: Tensor = fn(*args, **kwargs)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x150ca1ce1f30>
Traceback (most recent call last):
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 444, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 421, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 150, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 69, in put
    os.rename(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x151f34b1df30>
Traceback (most recent call last):
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 444, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 422, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_4d_kernel", __class__._4d_kernel)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 150, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 69, in put
    os.rename(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_4d_kernel.pickle.tmp' -> '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_4d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x1458b1706f80>
Traceback (most recent call last):
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 444, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 422, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_4d_kernel", __class__._4d_kernel)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 150, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 69, in put
    os.rename(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_4d_kernel.pickle.tmp' -> '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_4d_kernel.pickle'
