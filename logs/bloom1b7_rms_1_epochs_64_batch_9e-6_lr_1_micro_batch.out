[2024-07-01 16:50:06,409] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:06,409] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:06,409] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:27,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-01 16:50:27,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-01 16:50:27,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,659] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 64 --micro_train_batch_size 1 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset argilla/ultrafeedback-binarized-preferences-cleaned --dataset_probs 1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,659] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 64 --micro_train_batch_size 1 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset --dataset_probs 1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-07-01 16:50:27,659] [INFO] [runner.py:568:main] cmd = /gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ppi_scripts/train_rm.py --save_path ./ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch --save_steps 1000 --logging_steps 1 --eval_steps 5000 --train_batch_size 64 --micro_train_batch_size 1 --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --bf16 --max_epochs 1 --max_len 2048 --zero_stage 3 --learning_rate 9e-6 --dataset argilla/ultrafeedback-binarized-preferences-cleaned, --dataset_probs 1.0,1.0 --gradient_checkpointing --use_wandb True --wandb_org esfrankel-uw --wandb_group train_rm_bloom1b7 --wandb_project openrlhf_train_rm_ppi
[2024-07-01 16:50:30,519] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,593] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:30,596] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:31,273] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:31,273] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:31,273] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:31,273] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:31,274] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:31,274] [INFO] [launch.py:253:main] process 53865 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '1', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:31,345] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:31,345] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:31,345] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:31,345] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:31,345] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:31,346] [INFO] [launch.py:253:main] process 53866 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '1', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:31,346] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-07-01 16:50:31,346] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-07-01 16:50:31,346] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-07-01 16:50:31,346] [INFO] [launch.py:163:main] dist_world_size=1
[2024-07-01 16:50:31,346] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-07-01 16:50:31,347] [INFO] [launch.py:253:main] process 53867 spawned with command: ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '1', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned,', '--dataset_probs', '1.0,1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi']
[2024-07-01 16:50:46,347] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:46,350] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:46,350] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-01 16:50:52,830] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-01 16:50:52,830] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-01 16:50:52,830] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-01 16:50:52,831] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-01 16:50:54,340] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 230, num_elems = 5.26B
[2024-07-01 16:50:56,302] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 53865
[2024-07-01 16:50:56,302] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '1', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 1
[2024-07-01 16:50:56,371] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 53866
[2024-07-01 16:50:56,372] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '1', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', '--dataset_probs', '1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 2
[2024-07-01 16:50:56,373] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 53867
[2024-07-01 16:50:56,373] [ERROR] [launch.py:322:sigkill_handler] ['/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/bin/python', '-u', 'ppi_scripts/train_rm.py', '--local_rank=0', '--save_path', './ckpt/bloom1b7_rms_1_epochs_64_batch_9e-6_lr_1_micro_batch', '--save_steps', '1000', '--logging_steps', '1', '--eval_steps', '5000', '--train_batch_size', '64', '--micro_train_batch_size', '1', '--pretrain', 'OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--bf16', '--max_epochs', '1', '--max_len', '2048', '--zero_stage', '3', '--learning_rate', '9e-6', '--dataset', 'argilla/ultrafeedback-binarized-preferences-cleaned,', '--dataset_probs', '1.0,1.0', '--gradient_checkpointing', '--use_wandb', 'True', '--wandb_org', 'esfrankel-uw', '--wandb_group', 'train_rm_bloom1b7', '--wandb_project', 'openrlhf_train_rm_ppi'] exits with return code = 1
