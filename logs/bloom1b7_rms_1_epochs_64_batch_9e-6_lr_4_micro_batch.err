/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
usage: train_rm.py [-h] [--pretrain PRETRAIN] [--dataset DATASET]
                   [--dataset_probs DATASET_PROBS] [--save_path SAVE_PATH]
                   [--save_steps SAVE_STEPS] [--logging_steps LOGGING_STEPS]
                   [--eval_steps EVAL_STEPS] [--ckpt_path CKPT_PATH]
                   [--max_ckpt_num MAX_CKPT_NUM] [--max_ckpt_mem MAX_CKPT_MEM]
                   [--max_epochs MAX_EPOCHS]
                   [--micro_train_batch_size MICRO_TRAIN_BATCH_SIZE]
                   [--train_batch_size TRAIN_BATCH_SIZE]
                   [--max_samples MAX_SAMPLES] [--load_checkpoint]
                   [--max_norm MAX_NORM] [--max_len MAX_LEN] [--l2 L2]
                   [--loss LOSS] [--gradient_checkpointing] [--seed SEED]
                   [--local_rank LOCAL_RANK] [--zero_stage ZERO_STAGE]
                   [--bf16] [--learning_rate LEARNING_RATE] [--zpg ZPG]
                   [--adam_offload] [--flash_attn] [--compute_fp32_loss]
                   [--margin_loss] [--aux_loss_coef AUX_LOSS_COEF]
                   [--grad_accum_dtype GRAD_ACCUM_DTYPE]
                   [--disable_trace_cache] [--load_in_4bit]
                   [--lora_rank LORA_RANK] [--lora_alpha LORA_ALPHA]
                   [--lora_dropout LORA_DROPOUT]
                   [--target_modules [TARGET_MODULES ...]]
                   [--gradient_checkpointing_use_reentrant]
                   [--disable_fast_tokenizer] [--head_prefix HEAD_PREFIX]
                   [--prompt_key PROMPT_KEY] [--chosen_key CHOSEN_KEY]
                   [--rejected_key REJECTED_KEY]
                   [--input_template INPUT_TEMPLATE] [--apply_chat_template]
                   [--tokenizer_chat_template TOKENIZER_CHAT_TEMPLATE]
                   [--use_wandb USE_WANDB] [--wandb_org WANDB_ORG]
                   [--wandb_group WANDB_GROUP] [--wandb_project WANDB_PROJECT]
                   [--wandb_run_name WANDB_RUN_NAME]
train_rm.py: error: argument --dataset: expected one argument
[W socket.cpp:464] [c10d] The server socket has failed to listen on [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 184, in <module>
    train(args)
  File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 18, in train
    strategy.setup_distributed()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/utils/deepspeed.py", line 81, in setup_distributed
    deepspeed.init_distributed(timeout=timeout)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 121, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 149, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
    return func(*args, **kwargs)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to listen on [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 184, in <module>
[rank0]:     train(args)
[rank0]:   File "/mmfs1/gscratch/sewoong/ericsf/OpenRLHF/ppi_scripts/train_rm.py", line 22, in train
[rank0]:     model = get_llm_for_sequence_regression(
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/models/model.py", line 113, in get_llm_for_sequence_regression
[rank0]:     model = cls_class.from_pretrained(
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3626, in from_pretrained
[rank0]:     model = cls(config, *model_args, **model_kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/openrlhf/models/model.py", line 172, in __init__
[rank0]:     setattr(self, self.base_model_prefix, base_llm_model(config))
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 878, in __init__
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 878, in <listcomp>
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 680, in __init__
[rank0]:     self.mlp = LlamaMLP(config)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 193, in __init__
[rank0]:     self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 503, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 98, in __init__
[rank0]:     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
[rank0]:   File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 238, in wrapped_fn
[rank0]:     tensor: Tensor = fn(*args, **kwargs)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x14788d8e6ef0>
Traceback (most recent call last):
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 444, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 421, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 150, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/gscratch/sewoong/ericsf/miniconda3/envs/openrlhf/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 69, in put
    os.rename(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/mmfs1/home/ericsf/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
